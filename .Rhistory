s
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
# xgb_grid_1 = expand.grid( nrounds = 100, eta = c(0.01), max_depth = c(10),
#                           gamma = 1, colsample_bytree = 0.66, min_child_weight =1)
#
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(method = "cv", number = 5, verboseIter = TRUE, returnData = FALSE,
#                                returnResamp = "all", # save losses across all models
#                                classProbs = TRUE,  # set to TRUE for AUC to be computed
#                                summaryFunction = twoClassSummary,
#                                allowParallel = TRUE
# )
#
# xgb_train_1 = train(QuoteConversion_Flag~., data = train[1:1000, -1],
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree"
# )
stopCluster(cl)
Sys.time() - s
install.packages('xgboost/R-package/', repos=NULL, type='source')
cl <- makeCluster(detectCores())
registerDoParallel(cl)
s <- Sys.time()
s
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
# xgb_grid_1 = expand.grid( nrounds = 100, eta = c(0.01), max_depth = c(10),
#                           gamma = 1, colsample_bytree = 0.66, min_child_weight =1)
#
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(method = "cv", number = 5, verboseIter = TRUE, returnData = FALSE,
#                                returnResamp = "all", # save losses across all models
#                                classProbs = TRUE,  # set to TRUE for AUC to be computed
#                                summaryFunction = twoClassSummary,
#                                allowParallel = TRUE
# )
#
# xgb_train_1 = train(QuoteConversion_Flag~., data = train[1:1000, -1],
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree"
# )
stopCluster(cl)
Sys.time() - s
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
library(xgboost)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", #nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
setwd("~/Dropbox/Data project/Kaggle Homesite")
library(readr)
library(xgboost)
library(doParallel)
library(caret)
set.seed(308)
cat("reading the train and test data\n")
train <- read_csv("input/train.csv")
test  <- read_csv("input/test.csv")
# There are some NAs in the integer columns so conversion to zero
train[is.na(train)]   <- -1
test[is.na(test)]   <- -1
# cat("train data column names and details\n")
# names(train)
# str(train)
# summary(train)
# cat("test data column names and details\n")
# names(test)
# str(test)
# summary(test)
# seperating out the elements of the date column for the train set
train$month <- as.integer(format(train$Original_Quote_Date, "%m"))
train$year <- as.integer(format(train$Original_Quote_Date, "%y"))
train$day <- weekdays(as.Date(train$Original_Quote_Date))
# removing the date column
train <- train[,-c(2)]
# seperating out the elements of the date column for the train set
test$month <- as.integer(format(test$Original_Quote_Date, "%m"))
test$year <- as.integer(format(test$Original_Quote_Date, "%y"))
test$day <- weekdays(as.Date(test$Original_Quote_Date))
# removing the date column
test <- test[,-c(2)]
feature.names <- names(train)[c(3:301)]
# cat("Feature Names\n")
# feature.names
cat("assuming text variables are categorical & replacing them with numeric ids\n")
for (f in feature.names) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]], test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]]  <- as.integer(factor(test[[f]],  levels=levels))
}
}
# cat("train data column names after slight feature engineering\n")
# names(train)
# cat("test data column names after slight feature engineering\n")
# names(test)
set.seed(9)
gc()
tra<-train[,feature.names]
#tra<-tra[,c(1:50,65:105,110:165,180:230,245:290)]
dim(tra)
dim(test)
nrow(train)
h<-sample(nrow(train),100000)
dval<-xgb.DMatrix(data=data.matrix(tra[h,]),label=train$QuoteConversion_Flag[h])
#dtrain<-xgb.DMatrix(data=data.matrix(tra[-h,]),label=train$QuoteConversion_Flag[-h])
dtrain<-xgb.DMatrix(data=data.matrix(tra[,]),label=train$QuoteConversion_Flag)
watchlist<-list(val=dval,train=dtrain)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
s <- Sys.time()
s
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", #nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
# xgb_grid_1 = expand.grid( nrounds = 100, eta = c(0.01), max_depth = c(10),
#                           gamma = 1, colsample_bytree = 0.66, min_child_weight =1)
#
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(method = "cv", number = 5, verboseIter = TRUE, returnData = FALSE,
#                                returnResamp = "all", # save losses across all models
#                                classProbs = TRUE,  # set to TRUE for AUC to be computed
#                                summaryFunction = twoClassSummary,
#                                allowParallel = TRUE
# )
#
# xgb_train_1 = train(QuoteConversion_Flag~., data = train[1:1000, -1],
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree"
# )
stopCluster(cl)
Sys.time() - s
detach("package:doParallel", unload=TRUE)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 4,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 4,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
install.packages(“Rcpp”)
install.packages('Rcpp')
install.packages('xgboost/R-package/', type='source')
setwd("~/")
install.packages('xgboost/R-package/', type='source')
install.packages('xgboost/R-package/', repos=NULL, type='source')
devtools::install_github("hadley/devtools")
install_github("hadley/devtools")
install.packages("devtools")
devtools::install_local('xgboost/', subdir = 'R-package')
is_using_multithread()
install.packages('mxnet/R-package/', repos=NULL, type='source')
devtools::install_local('mxnet/', subdir = 'R-package')
library(mxnet)
devtools::install_local('mxnet/', subdir = 'R-package')
install.packages('mxnet/R-package/', repos=NULL, type='source')
install.packages('mxnet/R-package/', repos=NULL, type='source')
library(mxnet)
demo(topic = "basic_bench", package = "mxnet")
print(paste0(“Finish prediction… accuracy=”, accuracy(label, pred)))
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
install.packages("forecast")
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
library(forecast)
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
demo(topic = "basic_bench", package = "mxnet")
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
demo(topic = "basic_model", package = "mxnet")
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
library(xgboost)
devtools::install_local('xgboost/', subdir = 'R-package')
setwd("~/")
devtools::install_local('xgboost/', subdir = 'R-package')
library(xgboost)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
dtrain<-xgb.DMatrix(data=data.matrix(tra[,]),label=train$QuoteConversion_Flag)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
?n
library(ggmap)
library(ggplot2)
library(dplyr)
library(readr)
?n
install.packages('devtools', repo=\ 'https://cran.rstudio.com')
install.packages('devtools', repo=\'https://cran.rstudio.com')
install.packages('devtools', repo='https://cran.rstudio.com')
library(mxnet)
Sys.getenv("LD_LIBRARY_PATH")
Sys.getenv()
dyn.load('/usr/local/cuda/lib/libcudart.7.5.dylib')
dyn.load('/usr/local/cuda/lib/libcublas.dylib')
dyn.load('/usr/local/cuda/lib/libcurand.dylib')
Sys.getenv("LD_LIBRARY_PATH")
Sys.getenv("DYLD_LIBRARY_PATH")
dyn.load('/usr/local/cuda/lib/libcudart.7.5.dylib')
dyn.load('/usr/local/cuda/lib/libcublas.dylib')
dyn.load('/usr/local/cuda/lib/libcurand.dylib')
?dyn.load
?n.load
library(mxnet)
require(mxnet)
context("ndarray")
test_that("element-wise calculation for vector", {
x = 1:10
mat = mx.nd.array(as.array(x), mx.cpu(0))
expect_equal(x, as.array(mat))
expect_equal(x+1, as.array(mat+1))
expect_equal(x-10, as.array(mat-10))
expect_equal(x*20, as.array(mat*20))
expect_equal(x/3, as.array(mat/3), tolerance = 1e-5)
expect_equal(-1-x, as.array(-1-mat))
expect_equal(-5/x, as.array(-5/mat), tolerance = 1e-5)
expect_equal(x+x, as.array(mat+mat))
expect_equal(x/x, as.array(mat/mat))
expect_equal(x*x, as.array(mat*mat))
expect_equal(x-x, as.array(mat-mat))
expect_equal(as.array(1-mat), as.array(1-mat))
})
test_that("element-wise calculation for matrix", {
x = matrix(1:4, 2, 2)
mat = mx.nd.array(as.array(x), mx.cpu(0))
expect_equal(x, as.array(mat))
expect_equal(x+1, as.array(mat+1))
expect_equal(x-10, as.array(mat-10))
expect_equal(x*20, as.array(mat*20))
expect_equal(x/3, as.array(mat/3), tolerance = 1e-5)
expect_equal(-1-x, as.array(-1-mat))
expect_equal(-5/x, as.array(-5/mat), tolerance = 1e-5)
expect_equal(x+x, as.array(mat+mat))
expect_equal(x/x, as.array(mat/mat))
expect_equal(x*x, as.array(mat*mat))
expect_equal(x-x, as.array(mat-mat))
expect_equal(as.array(1-mat), as.array(1-mat))
})
test_that("ndarray ones, zeros, save and load", {
expect_equal(rep(0, 10), as.array(mx.nd.zeros(10)))
expect_equal(matrix(0, 10, 5), as.array(mx.nd.zeros(c(10, 5))))
expect_equal(rep(1, 10), as.array(mx.nd.ones(10)))
expect_equal(matrix(1, 10, 5), as.array(mx.nd.ones(c(10, 5))))
mat = mx.nd.array(1:20)
mx.nd.save(mat, 'temp.mat')
mat2 = mx.nd.load('temp.mat')
expect_true(is.mx.ndarray(mat2[[1]]))
expect_equal(as.array(mat), as.array(mat2[[1]]))
})
library(testthat)
library(mxnet)
test_check("mxnet")
a <- mx.nd.zeros(c(2, 3)) # create a 2-by-3 matrix on cpu
b <- mx.nd.zeros(c(2, 3), mx.cpu()) # create a 2-by-3 matrix on cpu
c <- mx.nd.zeros(c(2, 3), mx.gpu(0)) # create a 2-by-3 matrix on gpu 0, if you have CUA enabled.
a <- mx.nd.zeros(c(2, 3)) # create a 2-by-3 matrix on cpu
b <- mx.nd.zeros(c(2, 3), mx.cpu()) # create a 2-by-3 matrix on cpu
#c <- mx.nd.zeros(c(2, 3), mx.gpu(0)) # create a 2-by-3 matrix on gpu 0, if you have CUA enabled.
shotSel.dist <<- shot.pt %>%
group_by(ShotDist) %>%
summarise(totalFGA = sum(totalFGA)) %>%
mutate(perc = totalFGA/sum(totalFGA), y.breaks = cumsum(perc) - perc/2) %>%
slice(c(1, 8, 2:7))
shiny::runApp('Dropbox/Data project/NBAstat/app')
View(shotSel.dist)
View(shotSel.def)
shotSel.dist <<- shot.pt %>%
group_by(ShotDist) %>%
summarise(totalFGA = sum(totalFGA)) %>%
mutate(perc = totalFGA/sum(totalFGA), y.breaks = cumsum(perc) - perc/2) %>%
slice(c(1, 8, 2:7))
shiny::runApp('Dropbox/Data project/NBAstat/app')
View(shotSel.dist)
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
rm(list = ls())
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
library(ggplot2)
library(knitr)
library(dplyr)
data(ToothGrowth)
kable(head(ToothGrowth))
kable(summary(ToothGrowth))
ggplot(ToothGrowth, aes(x = factor(dose), y = len, fill = factor(supp))) +
geom_boxplot() +
ylab('Tooth length') + xlab('Dose in milligrams/day') +
ggtitle('Impact of Dose and Supplement Type') +
theme_bw(base_size = 15) + scale_fill_discrete(name="Supplement type")+
theme(legend.position = c(0.2, 0.8))
anova <- aov(len ~ supp * dose, data=toothGrowth)
summary(anova)
anova <- aov(len ~ supp * dose, data=ToothGrowth)
summary(anova)
TukeyHSD(anova)
TukeyHSD(anova)
anova <- aov(len ~ supp * factor(dose), data=ToothGrowth)
summary(anova)
TukeyHSD(anova)
TukeyHSD(anova)$supp
TukeyHSD(anova)$factor(dose)
TukeyHSD(anova)$supp
TukeyHSD(anova)$'factor(dose)'
TukeyHSD(anova)$supp
TukeyHSD(anova)$'factor(dose)'
?group_bt
?group_by
eq.url <- "http://cmmserv.mrl.illinois.edu/microfabschedule/guest/npMonthView.asp?QM=1&QY=2016&GI=0"
eq <- htmlTreeParse(eq.url, error=function(...){}, useInternalNodes = TRUE)
eq <- xpathSApply(eq,"//select[@name ='GI']/option",xmlValue)
eq.df <- data.frame(GI = seq(0, 38, by = 1), equipment = eq)
head(eq.df)
library(XML)
library(dplyr)
library(ggplot2)
library(knitr)
library(lubridate)
eq.url <- "http://cmmserv.mrl.illinois.edu/microfabschedule/guest/npMonthView.asp?QM=1&QY=2016&GI=0"
eq <- htmlTreeParse(eq.url, error=function(...){}, useInternalNodes = TRUE)
eq <- xpathSApply(eq,"//select[@name ='GI']/option",xmlValue)
eq.df <- data.frame(GI = seq(0, 38, by = 1), equipment = eq)
head(eq.df)
View(eq.df)
swirl()
library(swirl)
swirl()
0.997*0.001
0.003*0.999
5
0.003*0.999
info()
0.015*0.999
0.997*0.001/(0.997*0.001+0.015*0.999)
3.5
expect_dice()
expect_dice(1)
expect_dice
dice_high
expect_dice(dice_high)
expect_dice(dice_low)
0.5*(edh+edl)
integrate(myfunc, 0, 2)
spop
mean(spop)
allsam
apply(allsam, 1, mean)
mean(smeans)
dice_sqr
ex2_fair <- dice_sqr*dice_fair
ex2_fair <- sum(dice_sqr*dice_fair)
ex2_fair-3.5^2
sum(dice_sqr*dice_high)-edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif(10000),1000),1,mean))
sd(apply(matrix(poisson(4,10000),1000),1,mean))
sd(apply(matrix(poisson(4),1000),1,mean))
sd(apply(matrix(rpoisson(4),1000),1,mean))
sd(apply(matrix(rpois(4),1000),1,mean))
sd(apply(matrix(rpois(lambda = 4),1000),1,mean))
sd(apply(matrix(rpois(lambda = 4, 1000),1000),1,mean))
sd(apply(matrix(rpois(lambda = 4, 10000),1000),1,mean))
sd(apply(matrix(rpois(lambda = 4, 10000),10),1,mean))
sd(apply(matrix(rpois(lambda = 4, 100),10),1,mean))
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/2/sqrt(10)
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
library(caret)
?sigma
detach("package:stats", unload=TRUE)
library("stats", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
?stats::sigma
?sigma
?nearZeroVar
library(RMySQL)
ucscDb <- dbConnect(MySQL(),user="root", password='j989mjj8',
host="localhost")
result <- dbGetQuery(ucscDb,"show databases;")
View(result)
dbDisconnect(ucscDb)
ucscDb <- dbConnect(MySQL(),user="root", password='j989mjj8', db='nytaxi',
host="localhost")
dbDisconnect(ucscDb)
hg19 <- dbConnect(MySQL(),user="root", password='j989mjj8', db='nytaxi',
host="localhost")
allTables <- dbListTables(hg19)
length(allTables)
head(allTables)
dbListFields(hg19,allTables[1])
dbGetQuery(hg19, paste0("select count(*) from ", allTables[2]))
q <- dbSendQuery(hg19, paste0("select * from ", allTables[2]))
table <- fetch(q, 10)
table
dbDisconnect(hg19)
View(table)
library(mxnet)
setwd("~/Dropbox/Data project/EV station")
install.packages("amap")
library(ggmap)
library(grid)
library(dplyr)
library(ggplot2)
library(broom)
library(amap)
install.packages(broom)
install.packages("broom")
library(broom)
time.df <- read.csv('data/1Timedcar2go_week.csv', header = T)
set.seed(18)
wss <- data.frame(clusterNo = seq(1,50), wss = rep(0, 50))
for (i in 1:50){
clust.k <-time.df %>% select(Longitude, Latitude) %>% Kmeans(i, iter.max=500, method="manhattan")
wss$wss[i] <- clust.k$tot.withinss
}
clust.k <-time.df %>% select(Longitude, Latitude) %>% Kmeans(50, iter.max=500, method="manhattan")
for (i in 1:50){
clust.k <-time.df %>% select(Longitude, Latitude) %>% Kmeans(50, iter.max=500, method="manhattan")
wss$wss[i] <- sum(clust.k$withinss)
}
ggplot(wss)+geom_point(aes(clusterNo, wss), size = 4, shape = 1, color='#009E73')+
xlab('No. of Centroids') + ylab('WSS') +
theme_bw(18)
clust <- clust.k
ggmap(get_map(location = 'austin', zoom = 12), extent = 'device')+
geom_point(data=augment(clust, time.df),
aes(x = Longitude, y = Latitude, color = .cluster), alpha =0.1, size = 4) +
geom_point(aes(Longitude, Latitude), data = data.frame(clust$centers), size = 15, shape = 'x') +
theme(legend.position = 'none')
p3<- ggmap(get_map(location = 'austin', zoom = 12), extent = 'device')+
geom_point(data=augment(clust, time.df),
aes(x = Longitude, y = Latitude, color = .cluster), alpha =0.1, size = 4) +
geom_point(aes(Longitude, Latitude), data = data.frame(clust$centers), size = 15, shape = 'x') +
theme(legend.position = 'none')
p3.2<- ggmap(get_map('domain, austin', zoom = 15), extent = 'device')+
geom_point(data=augment(clust, time.df),
aes(x = Longitude, y = Latitude, color = .cluster), alpha =0.1, size = 4) +
geom_point(aes(Longitude, Latitude), data = data.frame(clust$centers), size = 15, shape = 'x') +
ggtitle('The Domain')+
theme(legend.position = 'none',
plot.title = element_text(size = rel(2)),
panel.border = element_rect(colour = "black", fill = NA, size=2))
plot_inset('6.png', p3, p3.2)
plot_inset <- function(name, p1, p2){
png(name, width=1280, height=1280)
grid.newpage()
v1<-viewport(width = 1, height = 1, x = 0.5, y = 0.5) #plot area for the main map
v2<-viewport(width = 0.2, height = 0.2, x = 0.18, y = 0.83) #plot area for the inset map
print(p1,vp=v1)
print(p2,vp=v2)
dev.off()
}
plot_inset('6.png', p3, p3.2)
